---
title: Getting Started with Platform Automation for PCF
owner: PCF Platform Automation
---

## <a id='requirements'></a> Requirements

* Deployed Concourse
* Docker to build a docker image
* Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio)
* Pivnet access to [Platform Automation](https://network.pivotal.io/products/platform-automation/)

<p class="note">
  <strong>Note</strong>: The Platform Automation for PCF is based on Concourse CI, it is recommanded that you
  have some faimilarity with Conocurse before started. If you are new to Concourse, 
  <a href="https://docs.pivotal.io/p-concourse/3-0/guides.html">Concourse CI Tutorials</a> would be a good place to start.
</p>

## <a id='setup'></a> Setup

1. Download the latest version of [Platform Automation](https://network.pivotal.io/products/platform-automation/) from Pivnet.
   You will need:
   * `Concourse Tasks`
   * `Self-Service Image Resources`
   
1. Unzip the `Self-Service Image Resources` file.

    ```bash
    unzip platform-automation-image-resources-*.zip
    cd image-resources/
    ```
   
1. Build the docker image for Concourse CI.

    ```bash
    ./create-docker-image.sh
    ```
    
    This created a file named `platform-automation-image-*.tgz`.
    
    Store the above file in a blobstore that can be access via a Concourse pipeline.
    It does not require a local docker registry.
    It can be accessed via a blobstore, too.
   
1. Store the `platform-automation-tasks-*.zip` in a blobstore that can be accessed via a Concourse pipeline.

1. Next we'll create a test pipeline to see if the assets can be accessed correctly.
   This pipeline runs a test task, which ensures that all the parts work correctly. 
   (the example pipeline assumes s3 as the blobstore)
   
    ```
    resources:
    - name: platform-automation-tasks-s3
      type: s3
      source:
        access_key_id: ((access_key_id))
        secret_access_key: ((secret_access_key))
        region_name: ((region))
        bucket: ((bucket))
        regexp: platform-automation-tasks-(.*).zip
    
    - name: platform-automation-image-s3
      type: s3
      source:
        access_key_id: ((access_key_id))
        secret_access_key: ((secret_access_key))
        region_name: ((region))
        bucket: ((bucket))
        regexp: platform-automation-image-(.*).tgz
    
    jobs:
    - name: test-resources
      plan:
      - aggregate:
        - get: platform-automation-tasks-s3
          params:
            unpack: true
        - get: platform-automation-image-s3
          params:
            unpack: true
      - task: test-resources
        image: platform-automation-image-s3
        file: platform-automation-tasks-s3/tasks/test.yml
    ```
    
    Fill in the S3 resource credentials and set the above pipeline on your Concourse instance.
<p class="note">
  <strong>Note</strong>: The pipeline can use any blobstore.
  We choose S3 because the resource natively supported by Concourse.
  S3 resource also supports S3-compatible blobstores (e.g. minio).
  See <a href="https://github.com/concourse/s3-resource#source-configuration">S3 Resource</a>
  for more information. If you want to use other blobstore, you need to provide a custom
   <a href="https://concourse-ci.org/resource-types.html">resource type</a> .
</p>

# <a id='generate-configuration'></a>Generating Configuration
Many of the tasks in Platform Automation for PCF require configuration.
There are four types of config files used by Platform Automation for PCF:
For the Ops Manager VM itself, there’s:

* Ops Manager VM Configuration
* Ops Manager Authentication Configuration

For the things deployed by Ops Manager, there’s:

* Director Configuration
* Product Configuration

Which of these you’ll need depends on what you’re automating.
How you generate them depends on what situation you’re working with.
In general, if you already have an Ops Manager, you won’t need Authentication Configuration,
as it’s only necessary upon initial deployment of an Ops Manager.
If your pipeline won’t be managing the Ops Manager VM itself, you don’t need Ops Manager Configuration either.

Both VM and Authentication config need to be created manually.
You can see the format and options for each file in the [`create-vm`][create-vm] and
[`configure-authentication`][configure-authentication] documentation, respectively.

Configuration for the director and other tiles is different,
in that their format and options varies by tile, and can be drawn from Ops Manager.

If you’re writing automation for an existing PCF Foundation
(or have access to one you want to base your configuration on),
you can extract Ops Manager Director and Product configuration from that foundation.
See the documentation for the [`staged-director-config`][staged-director-config] and [`staged-config`][staged-config] tasks.

If you don’t have an existing foundation to base your configuration on,
we recommend you manually configure your foundation through the GUI the first time,
then extract that configuration for future use.

# <a id='generate-auth-file'></a>Generating an Auth File
Almost all [`om`][om] commands require an auth file
to hit authenticated API endpoints.

The configuration for authentication has a dependency on either username/password

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'auth-configuration' %>

or client SAML setup.

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'saml-auth-configuration' %>

This configuration format matches the configuration for setting up authentication.
See the documentation for the [`configure-authentication`][configure-authentication]
or [`configure-saml-authentication`][configure-saml-authentication] task for details.

# <a id='manage-configration'></a>Managing Configuration, Auth, and State Files
To use all these files with the Concourse tasks that require them,
you need to make them available as Concourse Resources.
They’re all text files, and there are many resource types that can work for this - in our examples,
we use s3 compatible blobstores. As with the tasks and image,
you’ll need to upload them to a bucket and declare a resource in your pipeline for each file you need.

# <a id='your-own-pipeline'></a>Making Your Own Pipeline
If the example pipeline doesn’t work for you, that’s okay! It probably shouldn’t.
You know your environment and constraints, and we don’t.
We recommend you look at the tasks that make up the pipeline,
and see if they can be arranged such that they do what you need. 
If you have Platform Architects available, they can help you look at this problem.

Our example just illustrates the tasks and provides one possible starting place
- the suggested starting projects provide other starting places that make different choices.
Your pipeline is yours, not a fork of something we wrote.

If the tasks themselves don’t work for you, we’d like to hear from you.
We might be able to help you figure out how to make it work,
or we can use the feedback to improve the tasks so they’re a better fit for what you need.
If you need to write your own tasks in the meantime, our tasks are designed with clear interfaces,
and should be able to coexist in a pipeline with tasks from other sources, or custom tasks you develop yourself.

[concourse-documentation]: https://github.com/concourse/s3-resource
[om]: https://github.com/pivotal-cf/om
[create-vm]: ./task-reference.html#create-vm
[staged-director-config]: ./task-reference.html#staged-director-config
[staged-config]: ./task-reference.html#staged-config
[configure-authentication]: ./task-reference.html#configure-authentication
[configure-saml-authentication]: ./task-reference.html#configure-saml-authentication
