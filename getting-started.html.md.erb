---
title: Getting Started with Platform Automation for PCF
owner: PCF Platform Automation
---

## <a id='requirements'></a> Requirements

* Deployed Concourse
* Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio)
* Pivnet access to [Platform Automation](https://network.pivotal.io/products/platform-automation/)

<p class="note">
  <strong>Note</strong>: The Platform Automation for PCF is based on Concourse CI, it is recommended that you
  have some familiarity with Conocurse before getting started. If you are new to Concourse, 
  <a href="https://docs.pivotal.io/p-concourse/3-0/guides.html">Concourse CI Tutorials</a> would be a good place to start.
</p>

## <a id='setup'></a> Setup

1. Download the latest version of [Platform Automation](https://network.pivotal.io/products/platform-automation/) from Pivnet.
   You will need:
   * `Concourse Tasks`
   * `Docker Image for Concourse Tasks`
   
   <p class="note">
     <strong>Note</strong>: If the pivnet link does not work for you, you might not have access to the product! Please
     communicate this in the #pcf-automation slack channel until the project is GA 
   </p>
   
   
1. Store the `platform-automation-image-*.tgz` in a blobsotre that can be accessed via a Concourse pipeline.

1. Store the `platform-automation-tasks-*.zip` in a blobstore that can be accessed via a Concourse pipeline.

1. Next we'll create a test pipeline to see if the assets can be accessed correctly.
   This pipeline runs a test task, which ensures that all the parts work correctly. 
   (the example pipeline assumes s3 as the blobstore)
   
    ```
    resources:
    - name: platform-automation-tasks-s3
      type: s3
      source:
        access_key_id: ((access_key_id))
        secret_access_key: ((secret_access_key))
        region_name: ((region))
        bucket: ((bucket))
        regexp: platform-automation-tasks-(.*).zip
    
    - name: platform-automation-image-s3
      type: s3
      source:
        access_key_id: ((access_key_id))
        secret_access_key: ((secret_access_key))
        region_name: ((region))
        bucket: ((bucket))
        regexp: platform-automation-image-(.*).tgz
    
    jobs:
    - name: test-resources
      plan:
      - aggregate:
        - get: platform-automation-tasks-s3
          params:
            unpack: true
        - get: platform-automation-image-s3
          params:
            unpack: true
      - task: test-resources
        image: platform-automation-image-s3
        file: platform-automation-tasks-s3/tasks/test.yml
    ```
    
    Fill in the S3 resource credentials and set the above pipeline on your Concourse instance.
<p class="note">
  <strong>Note</strong>: The pipeline can use any blobstore.
  We choose S3 because the resource natively supported by Concourse.
  S3 resource also supports S3-compatible blobstores (e.g. minio).
  See <a href="https://github.com/concourse/s3-resource#source-configuration">S3 Resource</a>
  for more information. If you want to use other blobstore, you need to provide a custom
   <a href="https://concourse-ci.org/resource-types.html">resource type</a> .
</p>

## <a id='generate-env-file'></a>Generating an Env File
Almost all [`om`][om] commands require an env file
to hit authenticated API endpoints.

The configuration for authentication has a dependency on either username/password

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'env' %>

or client SAML setup.

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'env-uaa' %>

## <a id='generate-auth-file'></a>Generating an Auth File
These configuration formats match the configuration for setting up authentication.
See the documentation for the [`configure-authentication`][configure-authentication]
or [`configure-saml-authentication`][configure-saml-authentication] task for details.

The configuration for authentication has a dependency on either username/password

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'auth-configuration' %>

or client SAML setup.

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'saml-auth-configuration' %>

## <a id='generate-product-configuration'></a>Generating Product Configuration
To generate the configuration for a tile, you will need the following:

1. a running Ops Manager
  - this is accomplished by running the following tasks:
      - [create-vm]
      - [configure-authentication] or [configure-saml-authentication]
      - [configure-director]
      - [apply-director-changes]
  
1. The tile you wish to have a config file for needs to be [uploaded and staged] in the Ops Manager
environment

1. Configure the tile _manually_ within the Ops Manager UI (Instructions for PAS can be found
using the [Official PCF Documentation])

1. Run the following command to get the staged config:

```bash
docker run -it --rm -v $PWD:/workspace -w /workspace pcf-automation-image \
om --env ${ENV_FILE} staged-config --product-name ${PRODUCT_SLUG} --include-credentials
```

Where `${ENV_FILE}` is the [env file] required for all tasks, and `${PRODUCT_SLUG}` is the name of the product
downloaded from [pivnet]. The resulting file can then be parameterized, saved, and uploaded to a 
persistent blobstore(i.e. s3, gcs, azure blobstore, etc).

Alternatively, you can add the following task to your pipeline to generate and persist this for you:

<%= yield_for_code_snippet from: 'pivotal-cf/platform-automation', at: 'staged-config' %>

## <a id='manage-configration'></a>Managing Configuration, Auth, and State Files
To use all these files with the Concourse tasks that require them,
you need to make them available as Concourse Resources.
They’re all text files, and there are many resource types that can work for this - in our examples,
we use s3 compatible blobstores. As with the tasks and image,
you’ll need to upload them to a bucket and declare a resource in your pipeline for each file you need.

## <a id='your-own-pipeline'></a>Making Your Own Pipeline
If the example pipeline doesn’t work for you, that’s okay! It probably shouldn’t.
You know your environment and constraints, and we don’t.
We recommend you look at the tasks that make up the pipeline,
and see if they can be arranged such that they do what you need. 
If you have Platform Architects available, they can help you look at this problem.

Our example just illustrates the tasks and provides one possible starting place
- the suggested starting projects provide other starting places that make different choices.
Your pipeline is yours, not a fork of something we wrote.

If the tasks themselves don’t work for you, we’d like to hear from you.
We might be able to help you figure out how to make it work,
or we can use the feedback to improve the tasks so they’re a better fit for what you need.
If you need to write your own tasks in the meantime, our tasks are designed with clear interfaces,
and should be able to coexist in a pipeline with tasks from other sources, or custom tasks you develop yourself.

[apply-director-changes]: ./task-reference.html#apply-director-changes
[configure-authentication]: ./task-reference.html#configure-authentication
[configure-director]: ./task-reference.html#configure-director
[concourse-documentation]: https://github.com/concourse/s3-resource
[configure-saml-authentication]: ./task-reference.html#configure-saml-authentication
[create-vm]: ./task-reference.html#create-vm
[env file]: ./#generate-env-file
[Official PCF Documentation]: https://docs.pivotal.io/pivotalcf/installing/index.html
[om]: https://github.com/pivotal-cf/om
[pivnet]: https://network.pivotal.io
[staged-config]: ./task-reference.html#staged-config
[staged-director-config]: ./task-reference.html#staged-director-config
[uploaded-and-staged]: ./task-reference.html#upload-and-stage-product
